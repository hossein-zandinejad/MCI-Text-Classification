{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Project Overview\n",
    "\n",
    "This project focuses on classifying text data into different categories using a range of machine learning and deep learning techniques. The project encompasses the following key components:\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "- The project initiates with the loading of a JSONL dataset containing textual information.\n",
    "- Data preprocessing takes place, involving tokenization, lemmatization, and the removal of stopwords, rendering the text data suitable for subsequent natural language processing (NLP) and machine learning tasks.\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "- Word embeddings are employed to represent text data in the form of continuous vectors.\n",
    "- Both Word2Vec and FastText embeddings are applied to capture the semantic nuances within the text.\n",
    "\n",
    "## Model Development and Training\n",
    "\n",
    "- Various machine learning models are utilized for text classification, encompassing Multinomial Naive Bayes, Support Vector Classifier (SVC), Logistic Regression, Decision Tree Classifier, and Random Forest Classifier.\n",
    "- Additionally, a deep learning model in the form of an LSTM (Long Short-Term Memory) neural network is implemented.\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "- Each model is trained and assessed on the preprocessed text data.\n",
    "- Performance metrics, including accuracy, precision, recall, and F1-score, are computed to gauge the effectiveness of the models.\n",
    "\n",
    "## Ensemble Prediction\n",
    "\n",
    "- The project incorporates an ensemble approach, combining predictions from multiple models to make a consensus prediction for a given input text.\n",
    "- This ensemble method leverages the strengths of various models to enhance the overall classification accuracy.\n",
    "\n",
    "## Model Comparison\n",
    "\n",
    "- A bar chart is generated to provide a visual comparison of the accuracies achieved by different models.\n",
    "- This comparative analysis facilitates the evaluation and selection of the most suitable model for the text classification task.\n",
    "\n",
    "This project offers a comprehensive exploration of text classification techniques, spanning traditional machine learning algorithms and deep learning models. Furthermore, it showcases the potential of ensemble methods in improving classification accuracy. The project encompasses the complete text classification pipeline, encompassing data preprocessing, model development, evaluation, and comparative analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "This code cell imports various libraries for different tasks in the notebook. Here's a quick overview of the libraries:\n",
    "\n",
    "- **Standard Libraries**: Used for basic operations.\n",
    "- **Data Manipulation**: Essential for data handling.\n",
    "- **Natural Language Processing (NLP)**: Includes NLP and text processing tools.\n",
    "- **Data Visualization**: For creating visualizations.\n",
    "- **Machine Learning**: Required for machine learning tasks.\n",
    "- **Deep Learning**: Frameworks for deep learning.\n",
    "- **Text Processing**: For text feature extraction.\n",
    "- **Stopwords**: Downloads stopwords for text preprocessing.\n",
    "- **Initialize WordNet Lemmatizer**: Initializes a WordNet lemmatizer.\n",
    "\n",
    "These libraries will be used throughout the notebook for various data analysis and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hossein/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Data Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from tabulate import tabulate\n",
    "import gradio as gr\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    confusion_matrix,)\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Text Processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data\n",
    "\n",
    "In this code cell, data is loaded and prepared for further analysis. Here's a breakdown of what's happening:\n",
    "\n",
    "- **Initialize empty lists**: Three empty lists (`utterances`, `scenarios`, and `classes`) are created to store data from the dataset.\n",
    "\n",
    "- **Load the JSONL dataset**: The code opens a JSONL file named 'fa-IR.jsonl' and iterates through its lines. For each line, it loads a JSON record and checks if it contains the keys 'utt' (for utterance), 'scenario' (for scenario label), and 'partition' (for partition label). If these keys are present, the corresponding data is appended to the respective lists.\n",
    "\n",
    "- **Create a DataFrame**: Finally, a pandas DataFrame (`df`) is created from the collected data, with columns 'utt' for utterances, 'label' for scenario labels, and 'partition' for partition labels.\n",
    "\n",
    "This code sets the stage for data analysis and manipulation using the loaded dataset stored in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utt</th>\n",
       "      <th>label</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>این هفته ساعت پنج صبح بیدارم کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مرا جمعه ساعت نه صبح بیدار کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>یک زنگ هشدار را برای دو ساعت دیگر تنظیم کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ساکت</td>\n",
       "      <td>audio</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الی ساکت شو</td>\n",
       "      <td>audio</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          utt  label partition\n",
       "0             این هفته ساعت پنج صبح بیدارم کن  alarm      test\n",
       "1               مرا جمعه ساعت نه صبح بیدار کن  alarm     train\n",
       "2  یک زنگ هشدار را برای دو ساعت دیگر تنظیم کن  alarm     train\n",
       "3                                        ساکت  audio      test\n",
       "4                                 الی ساکت شو  audio     train"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances = []\n",
    "scenarios = []\n",
    "classes = []\n",
    "\n",
    "with open('fa-IR.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)\n",
    "        if 'utt' in record and 'scenario' in record and 'partition' in record:\n",
    "            utterances.append(record['utt'])\n",
    "            scenarios.append(record['scenario'])\n",
    "            classes.append(record['partition'])\n",
    "\n",
    "df = pd.DataFrame({'utt': utterances, 'label': scenarios, 'partition': classes})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "hovertemplate": "index=%{label}<br>value=%{value}<extra></extra>",
         "labels": [
          "train",
          "test",
          "dev"
         ],
         "legendgroup": "",
         "name": "",
         "showlegend": true,
         "type": "pie",
         "values": [
          11514,
          2974,
          2033
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Partition Distribution"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the count of unique labels with plotly_dark template\n",
    "label_counts = df['label'].value_counts()\n",
    "label_fig = px.bar(label_counts, x=label_counts.index, y=label_counts.values, labels={'x':'Label', 'y':'Count'}, title='Label Distribution')\n",
    "label_fig.update_layout(template=\"plotly_dark\")\n",
    "\n",
    "# Plot the count of unique partitions with plotly_dark template\n",
    "partition_counts = df['partition'].value_counts()\n",
    "partition_fig = px.bar(partition_counts, x=partition_counts.index, y=partition_counts.values, labels={'x':'Partition', 'y':'Count'}, title='Partition Distribution')\n",
    "partition_fig.update_layout(template=\"plotly_dark\")\n",
    "\n",
    "# Create a pie chart for label distribution\n",
    "label_pie_fig = px.pie(label_counts, values=label_counts.values, names=label_counts.index, title='Label Distribution')\n",
    "label_pie_fig.update_layout(template=\"plotly_dark\")\n",
    "\n",
    "# Create a pie chart for partition distribution\n",
    "partition_pie_fig = px.pie(partition_counts, values=partition_counts.values, names=partition_counts.index, title='Partition Distribution')\n",
    "partition_pie_fig.update_layout(template=\"plotly_dark\")\n",
    "\n",
    "# Display the plots\n",
    "# label_fig.show()\n",
    "# partition_fig.show()\n",
    "# label_pie_fig.show()\n",
    "# partition_pie_fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plt](images/01.png)\n",
    "![plt](images/02.png)\n",
    "![plt](images/03.png)\n",
    "![plt](images/04.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "In this code cell, the dataset is split into training and testing subsets. Here's a brief explanation:\n",
    "\n",
    "- **Create a train DataFrame**: A new DataFrame called `train_df` is created by selecting rows from the original DataFrame `df` where the 'partition' column has the value 'train'. This effectively isolates the training data.\n",
    "\n",
    "- **Create a test DataFrame**: Similarly, another DataFrame called `test_df` is created by selecting rows from the original DataFrame `df` where the 'partition' column has the value 'test'. This separates the testing data from the dataset.\n",
    "\n",
    "These DataFrames, `train_df` and `test_df`, now contain the training and testing subsets of the data, respectively. They can be used for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train DataFrame where partition is 'train'\n",
    "train_df = df[df['partition'] == 'train']\n",
    "\n",
    "# Create a test DataFrame where partition is 'test'\n",
    "test_df = df[df['partition'] == 'test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "In this code cell, text preprocessing steps are applied to the training and testing datasets. The following steps are performed:\n",
    "\n",
    "- **Define additional stopwords**: A list of additional stopwords specific to the Persian language is defined. These stopwords are words that are commonly removed from text during preprocessing because they often carry little meaningful information.\n",
    "\n",
    "- **Initialize the WordNet Lemmatizer**: The WordNet Lemmatizer from NLTK is initialized. Lemmatization is a process of reducing words to their base or root form.\n",
    "\n",
    "- **Define a preprocessing function**: A function called `preprocess_text` is defined. This function takes a text input, tokenizes it, lemmatizes the tokens, and removes stopwords from the text. The cleaned words are then joined back into a sentence.\n",
    "\n",
    "- **Apply preprocessing to train and test data**: The `preprocess_text` function is applied to the 'utt' column of both the training (`train_df`) and testing (`test_df`) DataFrames. The results are stored in new columns named 'Processed_utt' for both DataFrames.\n",
    "\n",
    "This preprocessing step is crucial for text data before it's used for machine learning or NLP tasks. It helps remove noise and reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stopwords\n",
    "stop_words = [\n",
    "    \"و\", \"در\", \"به\", \"از\", \"كه\", \"مي\", \"اين\", \"است\", \"را\", \"با\", \"هاي\",\n",
    "    \"براي\", \"آن\", \"يك\", \"شود\", \"شده\", \"خود\", \"ها\", \"كرد\", \"شد\", \"اي\",\n",
    "    \"تا\", \"كند\", \"بر\", \"بود\", \"گفت\", \"نيز\", \"وي\", \"هم\", \"كنند\", \"دارد\", \"ما\",\n",
    "    \"کن\", \"کرد\", \"کردن\", \"باش\", \"بود\", \"بودن\", \"شو\", \"شد\", \"شدن\", \"دار\",\n",
    "    \"داشت\", \"داشتن\", \"خواه\", \"خواست\", \"خواستن\", \"گوی\", \"گفت\", \"گفتن\",\n",
    "    \"گیر\", \"گرفت\", \"گرفتن\", \"آی\", \"آمد\", \"آمدن\", \"توان\", \"توانست\",\n",
    "    \"توانستن\", \"یاب\", \"یافت\", \"یافتن\", \"آور\", \"آورد\", \"آوردن\",\"دارم\",\"هستند\"]\n",
    "\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize the words and remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the cleaned words back into a sentence\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "train_df['Processed_utt'] = train_df['utt'].apply(preprocess_text)\n",
    "test_df['Processed_utt'] = test_df['utt'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utt</th>\n",
       "      <th>label</th>\n",
       "      <th>partition</th>\n",
       "      <th>Processed_utt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مرا جمعه ساعت نه صبح بیدار کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>train</td>\n",
       "      <td>مرا جمعه ساعت نه صبح بیدار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>یک زنگ هشدار را برای دو ساعت دیگر تنظیم کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>train</td>\n",
       "      <td>یک زنگ هشدار برای دو ساعت دیگر تنظیم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الی ساکت شو</td>\n",
       "      <td>audio</td>\n",
       "      <td>train</td>\n",
       "      <td>الی ساکت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>توقف</td>\n",
       "      <td>audio</td>\n",
       "      <td>train</td>\n",
       "      <td>توقف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>برای ده ثانیه متوقف کن</td>\n",
       "      <td>audio</td>\n",
       "      <td>train</td>\n",
       "      <td>برای ده ثانیه متوقف</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          utt  label partition  \\\n",
       "1               مرا جمعه ساعت نه صبح بیدار کن  alarm     train   \n",
       "2  یک زنگ هشدار را برای دو ساعت دیگر تنظیم کن  alarm     train   \n",
       "4                                 الی ساکت شو  audio     train   \n",
       "5                                        توقف  audio     train   \n",
       "6                      برای ده ثانیه متوقف کن  audio     train   \n",
       "\n",
       "                          Processed_utt  \n",
       "1            مرا جمعه ساعت نه صبح بیدار  \n",
       "2  یک زنگ هشدار برای دو ساعت دیگر تنظیم  \n",
       "4                              الی ساکت  \n",
       "5                                  توقف  \n",
       "6                   برای ده ثانیه متوقف  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding with Word2Vec\n",
    "\n",
    "In this code cell, Word2Vec embeddings are generated for the processed text data. Here's an explanation of the steps:\n",
    "\n",
    "- **Tokenize the processed text data**: The 'Processed_utt' column of both the training and testing DataFrames is tokenized using the `word_tokenize` function from NLTK. This step breaks down the text into individual words or tokens.\n",
    "\n",
    "- **Train Word2Vec models**: Two Word2Vec models (`word2vec_model` for training data and `word2vec_model_te` for testing data) are trained on the tokenized text. These models learn word embeddings in vector space, where each word is represented as a vector. You can adjust the model parameters (e.g., `vector_size`, `window`, `min_count`, `sg`) as needed for your specific task.\n",
    "\n",
    "- **Function to convert text to average Word2Vec vectors**: A function called `get_average_word2vec` is defined. It takes a list of tokens and a Word2Vec model as input and returns the average vector representation of the tokens. If a token is not found in the model (out-of-vocabulary word), it either generates a random vector or uses a zero vector, depending on the `generate_missing` parameter.\n",
    "\n",
    "- **Apply the function to create Word2Vec vectors**: The `get_average_word2vec` function is applied to each row of tokenized text in the 'Processed_utt' column of both the training and testing DataFrames. The resulting Word2Vec vectors are stored in new columns named 'Word2Vec' for both DataFrames.\n",
    "\n",
    "These Word2Vec embeddings capture semantic information about words in the text, making them suitable for various NLP tasks such as text classification or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the processed text data\n",
    "tokenized_text = train_df['Processed_utt'].apply(word_tokenize)\n",
    "tokenized_text_te = test_df['Processed_utt'].apply(word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model on your tokenized text (adjust parameters as needed)\n",
    "word2vec_model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, sg=0)\n",
    "word2vec_model_te = Word2Vec(tokenized_text_te, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Function to convert text to the average Word2Vec vectors\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=100):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "# Apply the function to create Word2Vec vectors for each tweet\n",
    "train_df['Word2Vec'] = tokenized_text.apply(lambda x: get_average_word2vec(x, word2vec_model.wv))\n",
    "test_df['Word2Vec'] = tokenized_text_te.apply(lambda x: get_average_word2vec(x, word2vec_model_te.wv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Labels\n",
    "\n",
    "In this code cell, categorical labels are encoded into numerical values using a `LabelEncoder`. Here's how it works:\n",
    "\n",
    "- **Initialize the LabelEncoder**: Two `LabelEncoder` instances are created, one for the \"Entity\" column and another for the \"Sentiment\" column. These encoders will map the unique categorical values to numerical labels.\n",
    "\n",
    "- **Fit and transform the categorical features**: The `fit_transform` method of the `LabelEncoder` is applied to both the training (`train_df`) and testing (`test_df`) DataFrames for their respective label columns. This step assigns a unique numerical label to each category in the \"Entity\" and \"Sentiment\" columns.\n",
    "\n",
    "The encoded labels are stored in new columns named 'label_Encoded' in both the training and testing DataFrames. These numerical labels can be used for training machine learning models, as many machine learning algorithms require numerical inputs for labels instead of categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder for \"Entity\" and \"Sentiment\"\n",
    "sentiment_label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categorical features to numerical values\n",
    "train_df['label_Encoded'] = sentiment_label_encoder.fit_transform(train_df['label'])\n",
    "test_df['label_Encoded'] = sentiment_label_encoder.fit_transform(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utt</th>\n",
       "      <th>label</th>\n",
       "      <th>partition</th>\n",
       "      <th>Processed_utt</th>\n",
       "      <th>Word2Vec</th>\n",
       "      <th>label_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مرا جمعه ساعت نه صبح بیدار کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>train</td>\n",
       "      <td>مرا جمعه ساعت نه صبح بیدار</td>\n",
       "      <td>[-0.06767874, 0.6883688, -0.08782967, -0.07785...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>یک زنگ هشدار را برای دو ساعت دیگر تنظیم کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>train</td>\n",
       "      <td>یک زنگ هشدار برای دو ساعت دیگر تنظیم</td>\n",
       "      <td>[-0.07333779, 0.87345713, -0.11739839, -0.0823...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الی ساکت شو</td>\n",
       "      <td>audio</td>\n",
       "      <td>train</td>\n",
       "      <td>الی ساکت</td>\n",
       "      <td>[-0.01489456, 0.10686286, -0.015576277, -0.012...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>توقف</td>\n",
       "      <td>audio</td>\n",
       "      <td>train</td>\n",
       "      <td>توقف</td>\n",
       "      <td>[-0.02130556, 0.11986162, -0.018047756, -0.017...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>برای ده ثانیه متوقف کن</td>\n",
       "      <td>audio</td>\n",
       "      <td>train</td>\n",
       "      <td>برای ده ثانیه متوقف</td>\n",
       "      <td>[-0.058250546, 0.60042155, -0.07817509, -0.076...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          utt  label partition  \\\n",
       "1               مرا جمعه ساعت نه صبح بیدار کن  alarm     train   \n",
       "2  یک زنگ هشدار را برای دو ساعت دیگر تنظیم کن  alarm     train   \n",
       "4                                 الی ساکت شو  audio     train   \n",
       "5                                        توقف  audio     train   \n",
       "6                      برای ده ثانیه متوقف کن  audio     train   \n",
       "\n",
       "                          Processed_utt  \\\n",
       "1            مرا جمعه ساعت نه صبح بیدار   \n",
       "2  یک زنگ هشدار برای دو ساعت دیگر تنظیم   \n",
       "4                              الی ساکت   \n",
       "5                                  توقف   \n",
       "6                   برای ده ثانیه متوقف   \n",
       "\n",
       "                                            Word2Vec  label_Encoded  \n",
       "1  [-0.06767874, 0.6883688, -0.08782967, -0.07785...              0  \n",
       "2  [-0.07333779, 0.87345713, -0.11739839, -0.0823...              0  \n",
       "4  [-0.01489456, 0.10686286, -0.015576277, -0.012...              1  \n",
       "5  [-0.02130556, 0.11986162, -0.018047756, -0.017...              1  \n",
       "6  [-0.058250546, 0.60042155, -0.07817509, -0.076...              1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utt</th>\n",
       "      <th>label</th>\n",
       "      <th>partition</th>\n",
       "      <th>Processed_utt</th>\n",
       "      <th>Word2Vec</th>\n",
       "      <th>label_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>این هفته ساعت پنج صبح بیدارم کن</td>\n",
       "      <td>alarm</td>\n",
       "      <td>test</td>\n",
       "      <td>این هفته ساعت پنج صبح بیدارم</td>\n",
       "      <td>[-0.032603335, 0.13861217, 0.021972628, -0.037...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ساکت</td>\n",
       "      <td>audio</td>\n",
       "      <td>test</td>\n",
       "      <td>ساکت</td>\n",
       "      <td>[-0.0020866692, -0.008983905, 0.0027714646, -0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>صورتی همان چیزی است که نیاز داریم</td>\n",
       "      <td>iot</td>\n",
       "      <td>test</td>\n",
       "      <td>صورتی همان چیزی که نیاز داریم</td>\n",
       "      <td>[-0.020410202, 0.068406925, 0.014616796, -0.02...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>و تاریک شده است</td>\n",
       "      <td>iot</td>\n",
       "      <td>test</td>\n",
       "      <td>تاریک</td>\n",
       "      <td>[0.00031434774, 0.0064995815, -0.009134296, -0...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>علی چراغ‌های اتاق خواب را خاموش کن</td>\n",
       "      <td>iot</td>\n",
       "      <td>test</td>\n",
       "      <td>علی چراغ‌های اتاق خواب خاموش</td>\n",
       "      <td>[-0.013984281, 0.05558011, 0.0059327064, -0.01...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   utt  label partition  \\\n",
       "0      این هفته ساعت پنج صبح بیدارم کن  alarm      test   \n",
       "3                                 ساکت  audio      test   \n",
       "8    صورتی همان چیزی است که نیاز داریم    iot      test   \n",
       "14                     و تاریک شده است    iot      test   \n",
       "19  علی چراغ‌های اتاق خواب را خاموش کن    iot      test   \n",
       "\n",
       "                    Processed_utt  \\\n",
       "0    این هفته ساعت پنج صبح بیدارم   \n",
       "3                            ساکت   \n",
       "8   صورتی همان چیزی که نیاز داریم   \n",
       "14                          تاریک   \n",
       "19   علی چراغ‌های اتاق خواب خاموش   \n",
       "\n",
       "                                             Word2Vec  label_Encoded  \n",
       "0   [-0.032603335, 0.13861217, 0.021972628, -0.037...              0  \n",
       "3   [-0.0020866692, -0.008983905, 0.0027714646, -0...              1  \n",
       "8   [-0.020410202, 0.068406925, 0.014616796, -0.02...              7  \n",
       "14  [0.00031434774, 0.0064995815, -0.009134296, -0...              7  \n",
       "19  [-0.013984281, 0.05558011, 0.0059327064, -0.01...              7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization with CountVectorizer and TF-IDF\n",
    "\n",
    "In this code cell, text data is vectorized using two different methods: CountVectorizer and TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer. Here's how it's done:\n",
    "\n",
    "- **Tokenize the text data**: The 'Processed_utt' column in both the training (`train_df`) and testing (`test_df`) DataFrames is tokenized using the `word_tokenize` function. This step splits the text into individual words or tokens.\n",
    "\n",
    "- **Prepare target labels**: The target labels for both training and testing datasets are stored in 'y_train' and 'y_test' variables, respectively.\n",
    "\n",
    "- **CountVectorizer**: \n",
    "  - The `CountVectorizer` from scikit-learn is initialized in the 'vectorizer' variable. This vectorizer converts text data into a matrix of word counts.\n",
    "  - It's applied to the 'Processed_utt' column of both the training and testing DataFrames, resulting in 'X_train' and 'X_test', which are matrices of word counts.\n",
    "\n",
    "- **TF-IDF Vectorizer**:\n",
    "  - The `TfidfVectorizer` from scikit-learn is initialized in the 'tfidf_vectorizer' variable. This vectorizer converts text data into a matrix of TF-IDF features.\n",
    "  - It's applied to the 'Processed_utt' column of both the training and testing DataFrames, resulting in 'X_train' and 'X_test', which are matrices of TF-IDF features.\n",
    "\n",
    "These text vectorization techniques transform text data into numerical format, making it suitable for machine learning models that require numerical input. CountVectorizer represents words as raw counts, while TF-IDF Vectorizer assigns weights to words based on their importance in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokenized_text'] = train_df['Processed_utt'].apply(word_tokenize)\n",
    "test_df['tokenized_text'] = test_df['Processed_utt'].apply(word_tokenize)\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df['Processed_utt'])\n",
    "X_test = vectorizer.transform(test_df['Processed_utt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df['Processed_utt'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['Processed_utt'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Multinomial Naive Bayes (MNB) Classifier\n",
    "\n",
    "In this code cell, a Multinomial Naive Bayes (MNB) classifier is used for text classification. Here's a breakdown of the steps:\n",
    "\n",
    "- **MNB Classifier Training and Prediction**:\n",
    "  - An MNB classifier (`mnb`) is initialized.\n",
    "  - The classifier is trained on the training data (`X_train` and `y_train`) using the `fit` method.\n",
    "  - Predictions are made on the testing data (`X_test`) using the `predict` method.\n",
    "  - Accuracy is calculated by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`).\n",
    "\n",
    "- **Display Results**:\n",
    "  - The accuracy of the MNB classifier is printed as a percentage.\n",
    "  - A classification report is generated, providing detailed evaluation metrics such as precision, recall, F1-score, and support for each class.\n",
    "\n",
    "- **Hyperparameter Tuning with Grid Search**:\n",
    "  - A parameter grid (`param_grid`) is defined, specifying different values for the alpha parameter of the MNB classifier.\n",
    "  - A new MNB classifier (`multinomial_nb`) is created.\n",
    "  - A `GridSearchCV` object (`grid_search`) is instantiated to perform grid search with 5-fold cross-validation and accuracy scoring.\n",
    "  - The grid search is performed using the training data to find the best alpha value.\n",
    "  - The best MNB classifier (`best_multinomial_nb`) and its corresponding best alpha value are extracted from the grid search results.\n",
    "\n",
    "- **Training and Evaluation with Best Model**:\n",
    "  - The best MNB classifier is trained on the full training set using the best alpha value.\n",
    "  - The best model is evaluated on the test set, and its accuracy is printed.\n",
    "\n",
    "This code demonstrates the process of training, evaluating, and tuning a Multinomial Naive Bayes classifier for text classification, along with reporting accuracy and detailed classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         alarm       0.98      0.59      0.74        96\n",
      "         audio       1.00      0.53      0.69        62\n",
      "      calendar       0.52      1.00      0.68       402\n",
      "       cooking       0.96      0.38      0.54        72\n",
      "      datetime       1.00      0.54      0.70       103\n",
      "         email       0.83      0.94      0.88       271\n",
      "       general       0.75      0.23      0.35       189\n",
      "           iot       0.96      0.95      0.95       220\n",
      "         lists       0.90      0.61      0.73       142\n",
      "         music       1.00      0.36      0.53        81\n",
      "          news       0.98      0.77      0.86       124\n",
      "          play       0.81      0.99      0.89       387\n",
      "            qa       0.75      0.86      0.81       288\n",
      "recommendation       0.93      0.55      0.69        94\n",
      "        social       1.00      0.56      0.72       106\n",
      "      takeaway       1.00      0.61      0.76        57\n",
      "     transport       0.93      0.90      0.91       124\n",
      "       weather       0.92      0.87      0.89       156\n",
      "\n",
      "      accuracy                           0.78      2974\n",
      "     macro avg       0.90      0.68      0.74      2974\n",
      "  weighted avg       0.83      0.78      0.77      2974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "accuracy_nb = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_nb:.2f}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha: 0.1\n",
      "MNB Test Accuracy with Best Model: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "\n",
    "# Create the MultinomialNB classifier\n",
    "multinomial_nb = MultinomialNB()\n",
    "\n",
    "# Create a GridSearchCV object to perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(multinomial_nb, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator (classifier) and its corresponding hyperparameters\n",
    "best_multinomial_nb = grid_search.best_estimator_\n",
    "best_alpha = best_multinomial_nb.alpha\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_multinomial_nb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_multinomial_nb.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Best Alpha: {best_alpha}')\n",
    "print(f'MNB Test Accuracy with Best Model: {accuracy_nb:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Random Forest Classifier\n",
    "\n",
    "In this code cell, a Random Forest Classifier is used for text classification. Here's a breakdown of the steps:\n",
    "\n",
    "- **Hyperparameter Tuning with Grid Search**:\n",
    "  - A parameter grid (`param_grid`) is defined, specifying different values for the 'n_estimators' (number of trees in the forest) and 'max_depth' (maximum depth of the tree) hyperparameters of the Random Forest Classifier.\n",
    "  - A Random Forest Classifier (`rf`) is initialized.\n",
    "  - A `GridSearchCV` object (`grid_search`) is instantiated to perform grid search with 5-fold cross-validation.\n",
    "  - The grid search is performed using the training data to find the best combination of hyperparameters.\n",
    "\n",
    "- **Training and Evaluation with Best Model**:\n",
    "  - The best Random Forest Classifier (`best_rf`) obtained from the grid search is trained on the full training set using the best hyperparameters.\n",
    "  - The best model is evaluated on the test set, and its accuracy is printed.\n",
    "\n",
    "This code demonstrates the process of hyperparameter tuning, training, and evaluating a Random Forest Classifier for text classification, along with reporting accuracy.\n",
    "\n",
    "Please note that there seems to be a minor typo in the accuracy print statement where it says \"MNB Test Accuracy with Best model.\" It should be \"RF Test Accuracy with Best Model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Test Accuracy with Best model: 0.85\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'RF Test Accuracy with Best model: {accuracy_rf:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Support Vector Classifier (SVC)\n",
    "\n",
    "In this code cell, a Support Vector Classifier (SVC) is used for text classification. Here's a breakdown of the steps:\n",
    "\n",
    "- **SVC Classifier Training and Prediction**:\n",
    "  - An SVC classifier (`svm_classifier`) with a linear kernel is initialized.\n",
    "  - The classifier is trained on the training data (`X_train` and `y_train`) using the `fit` method.\n",
    "  - Predictions are made on the testing data (`X_test`) using the `predict` method.\n",
    "  - Accuracy is calculated by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`).\n",
    "\n",
    "- **Display Results**:\n",
    "  - The accuracy of the SVC classifier is printed as a percentage.\n",
    "  - A classification report is generated, providing detailed evaluation metrics such as precision, recall, F1-score, and support for each class.\n",
    "\n",
    "- **Hyperparameter Tuning with Grid Search**:\n",
    "  - A parameter grid (`param_grid`) is defined, specifying different values for the 'C' (penalty parameter) and 'kernel' hyperparameters of the SVC classifier.\n",
    "  - A new SVC classifier (`SVC()`) is created.\n",
    "  - A `GridSearchCV` object (`grid_search`) is instantiated to perform grid search with 5-fold cross-validation.\n",
    "  - The grid search is performed using the training data to find the best combination of hyperparameters.\n",
    "\n",
    "- **Training and Evaluation with Best Model**:\n",
    "  - The best SVC classifier (`best_svm_classifier`) obtained from the grid search is trained on the full training set.\n",
    "  - The best model is evaluated on the test set, and its accuracy is printed.\n",
    "\n",
    "This code demonstrates the process of training, evaluating, and tuning a Support Vector Classifier for text classification, along with reporting accuracy and detailed classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         alarm       0.95      0.95      0.95        96\n",
      "         audio       0.92      0.87      0.89        62\n",
      "      calendar       0.85      0.94      0.89       402\n",
      "       cooking       0.89      0.78      0.83        72\n",
      "      datetime       0.98      0.85      0.91       103\n",
      "         email       0.96      0.95      0.96       271\n",
      "       general       0.53      0.55      0.54       189\n",
      "           iot       0.98      0.95      0.97       220\n",
      "         lists       0.95      0.84      0.89       142\n",
      "         music       0.91      0.77      0.83        81\n",
      "          news       0.93      0.82      0.87       124\n",
      "          play       0.94      0.96      0.95       387\n",
      "            qa       0.71      0.88      0.79       288\n",
      "recommendation       0.85      0.79      0.82        94\n",
      "        social       0.94      0.85      0.89       106\n",
      "      takeaway       0.98      0.81      0.88        57\n",
      "     transport       0.95      0.91      0.93       124\n",
      "       weather       0.96      0.90      0.93       156\n",
      "\n",
      "      accuracy                           0.88      2974\n",
      "     macro avg       0.90      0.85      0.87      2974\n",
      "  weighted avg       0.88      0.88      0.88      2974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy_svc = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_svc:.2f}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Test Accuracy with Best Model: 0.88\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_svm_classifier.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'SVC Test Accuracy with Best Model: {accuracy_svc:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Logistic Regression\n",
    "\n",
    "In this code cell, Logistic Regression is used for text classification. Here's a breakdown of the steps:\n",
    "\n",
    "- **Logistic Regression Classifier Training and Prediction**:\n",
    "  - A Logistic Regression classifier (`lr_classifier`) is initialized with a maximum number of iterations (`max_iter`) set to 1000.\n",
    "  - The classifier is trained on the training data (`X_train` and `y_train`) using the `fit` method.\n",
    "  - Predictions are made on the testing data (`X_test`) using the `predict` method.\n",
    "  - Accuracy is calculated by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`).\n",
    "\n",
    "- **Display Results**:\n",
    "  - The accuracy of the Logistic Regression classifier is printed as a percentage.\n",
    "  - A classification report is generated, providing detailed evaluation metrics such as precision, recall, F1-score, and support for each class.\n",
    "\n",
    "- **Hyperparameter Tuning with Grid Search**:\n",
    "  - A parameter grid (`param_grid`) is defined, specifying different values for the 'C' (inverse of regularization strength) hyperparameter of the Logistic Regression classifier.\n",
    "  - A new Logistic Regression classifier (`lr_classifier`) is created.\n",
    "  - A `GridSearchCV` object (`grid_search`) is instantiated to perform grid search with 5-fold cross-validation and accuracy scoring.\n",
    "  - The grid search is performed using the training data to find the best value for the 'C' hyperparameter.\n",
    "\n",
    "- **Training and Evaluation with Best Model**:\n",
    "  - The best Logistic Regression classifier (`best_lr_classifier`) obtained from the grid search is trained on the full training set using the best 'C' value.\n",
    "  - The best model is evaluated on the test set, and its accuracy is printed.\n",
    "\n",
    "This code demonstrates the process of training, evaluating, and tuning a Logistic Regression classifier for text classification, along with reporting accuracy and detailed classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         alarm       0.97      0.86      0.91        96\n",
      "         audio       0.94      0.81      0.87        62\n",
      "      calendar       0.84      0.95      0.89       402\n",
      "       cooking       0.91      0.72      0.81        72\n",
      "      datetime       0.96      0.83      0.89       103\n",
      "         email       0.96      0.94      0.95       271\n",
      "       general       0.59      0.49      0.54       189\n",
      "           iot       0.97      0.96      0.96       220\n",
      "         lists       0.94      0.85      0.89       142\n",
      "         music       0.92      0.72      0.81        81\n",
      "          news       0.93      0.77      0.85       124\n",
      "          play       0.93      0.96      0.94       387\n",
      "            qa       0.64      0.92      0.76       288\n",
      "recommendation       0.87      0.78      0.82        94\n",
      "        social       0.98      0.81      0.89       106\n",
      "      takeaway       0.98      0.81      0.88        57\n",
      "     transport       0.96      0.94      0.95       124\n",
      "       weather       0.94      0.88      0.91       156\n",
      "\n",
      "      accuracy                           0.87      2974\n",
      "     macro avg       0.90      0.83      0.86      2974\n",
      "  weighted avg       0.88      0.87      0.87      2974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_lr:.2f}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10\n",
      "LR Test Accuracy with Best Model: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Create the Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create a GridSearchCV object to perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(lr_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator (classifier) and its corresponding hyperparameters\n",
    "best_lr_classifier = grid_search.best_estimator_\n",
    "best_C = best_lr_classifier.C\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_lr_classifier.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Best C: {best_C}')\n",
    "print(f'LR Test Accuracy with Best Model: {accuracy_lr:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Decision Tree Classifier\n",
    "\n",
    "In this code cell, a Decision Tree Classifier is used for text classification. Here's a breakdown of the steps:\n",
    "\n",
    "- **Decision Tree Classifier Training and Prediction**:\n",
    "  - A Decision Tree Classifier (`dt_classifier`) is initialized.\n",
    "  - The classifier is trained on the training data (`X_train` and `y_train`) using the `fit` method.\n",
    "  - Predictions are made on the testing data (`X_test`) using the `predict` method.\n",
    "  - Accuracy is calculated by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`).\n",
    "\n",
    "- **Display Results**:\n",
    "  - The accuracy of the Decision Tree Classifier is printed as a percentage.\n",
    "  - A classification report is generated, providing detailed evaluation metrics such as precision, recall, F1-score, and support for each class.\n",
    "\n",
    "- **Hyperparameter Tuning with Grid Search**:\n",
    "  - A parameter grid (`param_grid`) is defined, specifying different values for the 'max_depth', 'min_samples_split', and 'min_samples_leaf' hyperparameters of the Decision Tree Classifier.\n",
    "  - A new Decision Tree Classifier (`DecisionTreeClassifier()`) is created.\n",
    "  - A `GridSearchCV` object (`grid_search`) is instantiated to perform grid search with 5-fold cross-validation.\n",
    "  - The grid search is performed using the training data to find the best combination of hyperparameters.\n",
    "\n",
    "- **Training and Evaluation with Best Model**:\n",
    "  - The best Decision Tree Classifier (`best_dt_classifier`) obtained from the grid search is trained on the full training set using the best hyperparameters.\n",
    "  - The best model is evaluated on the test set, and its accuracy is printed.\n",
    "\n",
    "This code demonstrates the process of training, evaluating, and tuning a Decision Tree Classifier for text classification, along with reporting accuracy and detailed classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         alarm       0.80      0.82      0.81        96\n",
      "         audio       0.79      0.81      0.80        62\n",
      "      calendar       0.78      0.82      0.80       402\n",
      "       cooking       0.78      0.74      0.76        72\n",
      "      datetime       0.88      0.79      0.83       103\n",
      "         email       0.89      0.90      0.89       271\n",
      "       general       0.38      0.47      0.42       189\n",
      "           iot       0.94      0.94      0.94       220\n",
      "         lists       0.82      0.80      0.81       142\n",
      "         music       0.80      0.69      0.74        81\n",
      "          news       0.80      0.71      0.75       124\n",
      "          play       0.91      0.89      0.90       387\n",
      "            qa       0.69      0.74      0.72       288\n",
      "recommendation       0.54      0.53      0.54        94\n",
      "        social       0.88      0.75      0.81       106\n",
      "      takeaway       0.88      0.74      0.80        57\n",
      "     transport       0.85      0.85      0.85       124\n",
      "       weather       0.90      0.82      0.86       156\n",
      "\n",
      "      accuracy                           0.79      2974\n",
      "     macro avg       0.79      0.77      0.78      2974\n",
      "  weighted avg       0.80      0.79      0.79      2974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "accuracy_dt = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_dt:.2f}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Test Accuracy with Best Model: 0.79\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_dt_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_dt_classifier.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'DT Test Accuracy with Best Model: {accuracy_dt:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with FastText Word Embeddings\n",
    "\n",
    "In this code cell, FastText word embeddings are used for text classification. Here's a breakdown of the steps:\n",
    "\n",
    "- **Load FastText Word Embeddings Model**:\n",
    "  - A pre-trained FastText word embeddings model (`fasttext_model`) is loaded using the `FastText.load_fasttext_format` function. This model contains pre-trained word vectors for the Persian language.\n",
    "\n",
    "- **Text to Embedding Conversion**:\n",
    "  - A function called `text_to_embedding` is defined. This function takes a text input, tokenizes it into words, and converts each word to its word vector using the FastText model.\n",
    "  - It filters out words that are not in the FastText model's vocabulary and computes the average word vector for the entire text.\n",
    "  - The result is an average word embedding for the input text.\n",
    "\n",
    "- **Convert Text Data to Word Embeddings**:\n",
    "  - The 'utt' column of both the training (`train_df`) and testing (`test_df`) DataFrames is processed to obtain FastText word embeddings. For each text in the DataFrame, the `text_to_embedding` function is applied to obtain a word embedding vector.\n",
    "  - These word embedding vectors are stored in `X_train_embeddings` and `X_test_embeddings`, respectively.\n",
    "\n",
    "- **Training with Word Embeddings**:\n",
    "  - The Logistic Regression classifier (`lr_classifier`) is trained on the training data with FastText word embeddings (`X_train_embeddings`) instead of traditional text features.\n",
    "\n",
    "- **Make Predictions and Evaluate**:\n",
    "  - Predictions are made on the test set using the trained model with word embeddings.\n",
    "  - Accuracy is calculated by comparing the predicted labels (`y_pred`) with the actual labels (`y_test`).\n",
    "\n",
    "- **Display Results**:\n",
    "  - The accuracy of the Logistic Regression classifier using FastText word embeddings is printed as a percentage.\n",
    "\n",
    "This code demonstrates the process of using pre-trained FastText word embeddings for text classification, allowing the model to leverage pre-trained word vectors to capture semantic information in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = FastText.load_fasttext_format('cc.fa.300.bin')\n",
    "\n",
    "def text_to_embedding(text, model, vector_size):\n",
    "    words = text.split()\n",
    "    # Filter words that are in the FastText model's vocabulary\n",
    "    valid_words = [word for word in words if word in model.wv]\n",
    "    if valid_words:\n",
    "        embeddings = [model.wv[word] for word in valid_words]\n",
    "        average_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        # If no valid words are found, create a zero vector\n",
    "        average_embedding = np.zeros(vector_size)\n",
    "    return average_embedding\n",
    "\n",
    "X_train_embeddings = [text_to_embedding(text, fasttext_model, vector_size=300) for text in train_df['utt']]\n",
    "X_test_embeddings = [text_to_embedding(text, fasttext_model, vector_size=300) for text in test_df['utt']]\n",
    "\n",
    "# Train the model on the FastText word embeddings\n",
    "lr_classifier.fit(X_train_embeddings, train_df['label'])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_classifier.predict(X_test_embeddings)\n",
    "\n",
    "# Evaluate the model and decode the labels as needed\n",
    "\n",
    "accuracy_lr_fsttext = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_lr_fsttext:.2f}')\n",
    "# print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with LSTM Neural Network\n",
    "\n",
    "In this code cell, a Long Short-Term Memory (LSTM) neural network is used for text classification. Here's a breakdown of the steps:\n",
    "\n",
    "- **Data Preparation**:\n",
    "  - Processed text data is extracted from the 'Processed_utt' column of both the training (`train_df`) and testing (`test_df`) DataFrames.\n",
    "  - Target labels are stored in `y_train` and `y_test`.\n",
    "\n",
    "- **Text Tokenization and Padding**:\n",
    "  - A `Tokenizer` is used to tokenize the text data and convert it into sequences of integers.\n",
    "  - The sequences are padded to a common maximum sequence length to ensure uniform input size for the neural network.\n",
    "\n",
    "- **Label Encoding**:\n",
    "  - A `LabelEncoder` is used to encode the target labels into numerical values.\n",
    "  - The encoded labels are converted to one-hot encoding to be used for multi-class classification.\n",
    "\n",
    "- **LSTM Model Construction**:\n",
    "  - A Sequential neural network model is created.\n",
    "  - It starts with an embedding layer to convert integer sequences into dense vectors.\n",
    "  - An LSTM layer with 128 units is added to capture sequential information.\n",
    "  - A dense layer with a softmax activation function is added to produce class probabilities.\n",
    "\n",
    "- **Model Compilation**:\n",
    "  - The model is compiled with categorical cross-entropy as the loss function and the Adam optimizer.\n",
    "  - Accuracy is chosen as a metric to monitor during training.\n",
    "\n",
    "- **Training**:\n",
    "  - The model is trained on the padded training data (`X_train_padded`) and one-hot encoded labels (`y_train_one_hot`) with 10 epochs and a batch size of 256.\n",
    "\n",
    "- **Evaluation**:\n",
    "  - The trained model is evaluated on the test data (`X_test_padded` and `y_test_one_hot`).\n",
    "  - Test loss and accuracy are printed to assess the model's performance.\n",
    "\n",
    "This code demonstrates the process of text classification using an LSTM neural network, including data preprocessing, model construction, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 5s 60ms/step - loss: 2.5358 - accuracy: 0.2653 - val_loss: 2.0200 - val_accuracy: 0.4159\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 1.3664 - accuracy: 0.6181 - val_loss: 0.8347 - val_accuracy: 0.7784\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 3s 58ms/step - loss: 0.4941 - accuracy: 0.8810 - val_loss: 0.5037 - val_accuracy: 0.8705\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.2514 - accuracy: 0.9393 - val_loss: 0.4472 - val_accuracy: 0.8786\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.1589 - accuracy: 0.9640 - val_loss: 0.4296 - val_accuracy: 0.8779\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.1115 - accuracy: 0.9759 - val_loss: 0.4442 - val_accuracy: 0.8773\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 2s 56ms/step - loss: 0.0880 - accuracy: 0.9812 - val_loss: 0.4829 - val_accuracy: 0.8719\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 3s 57ms/step - loss: 0.0681 - accuracy: 0.9851 - val_loss: 0.4535 - val_accuracy: 0.8749\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 3s 61ms/step - loss: 0.0542 - accuracy: 0.9882 - val_loss: 0.4776 - val_accuracy: 0.8753\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.0418 - accuracy: 0.9924 - val_loss: 0.5009 - val_accuracy: 0.8746\n",
      "93/93 [==============================] - 1s 5ms/step - loss: 0.5009 - accuracy: 0.8746\n",
      "Test Loss: 0.5008704662322998\n",
      "Test Accuracy: 0.874579668045044\n"
     ]
    }
   ],
   "source": [
    "# Extract processed text data\n",
    "X_train_text = train_df['Processed_utt']\n",
    "X_test_text = test_df['Processed_utt']\n",
    "\n",
    "# Target labels\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "max_sequence_length = max([len(seq) for seq in X_train_seq + X_test_seq])\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Build an LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256, input_length=max_sequence_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train_one_hot, validation_data=(X_test_padded, y_test_one_hot), epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy_nn = model.evaluate(X_test_padded, y_test_one_hot)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy_nn}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy Comparison Bar Chart\n",
    "\n",
    "In this code cell, a bar chart is created to visually compare the accuracies of different text classification models. Here's a breakdown of the steps:\n",
    "\n",
    "- **Model Names**:\n",
    "  - An array `model_names` is defined, containing the names of the models for which accuracies are being compared.\n",
    "\n",
    "- **Create a Bar Chart Trace**:\n",
    "  - A bar chart trace is created using Plotly's `go.Bar` function. The x-axis represents the model names (`model_names`), and the y-axis represents the corresponding accuracies.\n",
    "  - Each bar is color-coded differently to distinguish between models.\n",
    "\n",
    "- **Layout Configuration**:\n",
    "  - The layout for the plot is configured using Plotly's `go.Layout` function. It includes a title, axis labels, and a dark template for the plot.\n",
    "\n",
    "- **Create a Figure and Add the Trace**:\n",
    "  - A Plotly figure (`fig`) is created and initialized with the bar chart trace and layout.\n",
    "\n",
    "- **Display the Interactive Plot**:\n",
    "  - The interactive plot is displayed using `fig.show()`.\n",
    "\n",
    "This code allows for a visual comparison of model accuracies in a bar chart, making it easier to assess and compare the performance of different text classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names\n",
    "model_names = ['Random Forest', 'Neural Network', 'Naive Bayes', 'SVC', 'Decision Tree', 'Logistic Regression (FastText)']\n",
    "\n",
    "# Create a bar chart trace for accuracies\n",
    "trace = go.Bar(x=model_names, y=[accuracy_rf, accuracy_nn, accuracy_nb, accuracy_svc, accuracy_dt, accuracy_lr_fsttext],\n",
    "               marker=dict(color=['blue', 'green', 'red', 'purple', 'orange', 'pink']))\n",
    "\n",
    "# Create the layout for the plot\n",
    "layout = go.Layout(\n",
    "    title='Model Accuracy Comparison',\n",
    "    xaxis=dict(title='Model'),\n",
    "    yaxis=dict(title='Accuracy'),template='plotly_dark')\n",
    "\n",
    "\n",
    "# Create a figure and add the trace\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Display the interactive plot\n",
    "# fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![acuuracy](images/1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Ensemble Prediction\n",
    "\n",
    "In this code cell, an ensemble prediction for the category of an input text is made using multiple text classification models. Here's a breakdown of the steps:\n",
    "\n",
    "- **Input Text**:\n",
    "  - An input text is provided in the variable `input_text`. You can replace this text with your own input text that you want to classify.\n",
    "\n",
    "- **Text Preprocessing**:\n",
    "  - The input text is preprocessed using a preprocessing function (`preprocess_text`). This function tokenizes the text, removes stopwords, and performs lemmatization.\n",
    "\n",
    "- **Vectorization of Preprocessed Input**:\n",
    "  - The preprocessed input text (`preprocessed_input`) is vectorized using the same vectorizer that was used during training. In this case, TF-IDF vectorization is used (`tfidf_vectorizer.transform`).\n",
    "\n",
    "- **Prediction with Individual Models**:\n",
    "  - Predictions are made for the input text using three different models: Logistic Regression (`best_lr_classifier`), Support Vector Classifier (`best_svm_classifier`), and Decision Tree Classifier (`best_dt_classifier`).\n",
    "\n",
    "- **Ensemble Prediction with Majority Voting**:\n",
    "  - The predicted labels from the three models are combined into a list (`all_predicted_labels`).\n",
    "  - Majority voting is performed to determine the final predicted label. The label that occurs most frequently among the three predictions is selected as the final category.\n",
    "\n",
    "- **Print Predicted Category**:\n",
    "  - The final predicted category label is printed.\n",
    "\n",
    "This code demonstrates how to use an ensemble of multiple classification models to make a consensus prediction for the category of an input text, leveraging the strengths of different models to improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: news\n"
     ]
    }
   ],
   "source": [
    "# Replace 'input_text' with your actual input text\n",
    "input_text = \"مذاکرات ایران و آمریکا\"\n",
    "\n",
    "# Preprocess the input text (you need to implement your preprocessing)\n",
    "preprocessed_input = preprocess_text(input_text)\n",
    "\n",
    "# Vectorize the preprocessed input text using the same vectorizer used during training\n",
    "vectorized_input = tfidf_vectorizer.transform([preprocessed_input])  # Use your vectorizer (e.g., TF-IDF) here\n",
    "\n",
    "# Initialize lists to store predicted labels from different models\n",
    "predicted_labels_lr = best_lr_classifier.predict(vectorized_input)\n",
    "predicted_labels_svm = best_svm_classifier.predict(vectorized_input)\n",
    "predicted_labels_rf = best_dt_classifier.predict(vectorized_input)\n",
    "\n",
    "# Combine the predicted labels from different models into a single list\n",
    "all_predicted_labels = [predicted_labels_lr[0], predicted_labels_svm[0], predicted_labels_rf[0]]\n",
    "\n",
    "# Perform majority voting to determine the final label\n",
    "final_label = Counter(all_predicted_labels).most_common(1)[0][0]\n",
    "\n",
    "# Decode the final label back to its original category\n",
    "# final_category = sentiment_label_encoder.inverse_transform([final_label])[0]\n",
    "\n",
    "# Print the final category\n",
    "print(f'Predicted Category: {final_label}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Text Classifier Interface\n",
    "\n",
    "To make use of the text classification model developed in this project, you can interact with the user interface provided. Follow these steps to utilize the interface for predicting the category of a given text:\n",
    "\n",
    "1. **Launch the Interface**: Run the provided code to launch the interface. You can do this by executing the script or cell containing the code snippet.\n",
    "\n",
    "2. **Enter Text**: In the interface, you will find an input field labeled \"text.\" Enter the text you want to classify into this field.\n",
    "\n",
    "3. **Get Predicted Category**: After entering the text, click the interface's \"Submit\" or \"Predict\" button (depending on the interface design). The model will process the input text and predict its category based on the trained machine learning models.\n",
    "\n",
    "4. **View Predicted Category**: The predicted category will be displayed as the output of the interface. This represents the model's classification for the provided text.\n",
    "\n",
    "Feel free to experiment with different texts to observe how the model classifies them. This user-friendly interface simplifies the process of obtaining predictions from the text classification model.\n",
    "\n",
    "Remember that the accuracy of predictions depends on the quality of the training data and the performance of the underlying machine learning models used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize lists to store predicted labels from different models\n",
    "def predict_labels(text):\n",
    "\n",
    "    # Preprocess the input text (you need to implement your preprocessing)\n",
    "    preprocessed_input = preprocess_text(text)\n",
    "\n",
    "    # Vectorize the preprocessed input text using the same vectorizer used during training\n",
    "    vectorized_input = tfidf_vectorizer.transform([preprocessed_input])  # Use your vectorizer (e.g., TF-IDF) here\n",
    "\n",
    "    # Initialize lists to store predicted labels from different models\n",
    "    predicted_labels_lr = best_lr_classifier.predict(vectorized_input)\n",
    "    predicted_labels_svm = best_svm_classifier.predict(vectorized_input)\n",
    "    predicted_labels_rf = best_dt_classifier.predict(vectorized_input)\n",
    "\n",
    "    # Combine the predicted labels from different models into a single list\n",
    "    all_predicted_labels = [predicted_labels_lr[0], predicted_labels_svm[0], predicted_labels_rf[0]]\n",
    "\n",
    "    # Perform majority voting to determine the final label\n",
    "    final_label = Counter(all_predicted_labels).most_common(1)[0][0]\n",
    "\n",
    "    return final_label\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict_labels,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=\"text\",\n",
    "    title=\"Text Classifier\",\n",
    "    description=\"Enter a text and get the predicted category.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradio](images/2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
